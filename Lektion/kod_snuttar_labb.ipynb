{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kod snutt till labb_1 om jag hellre vill plotta som en funktion\n",
    "\n",
    "def plot_category_vs_cardio(df, category, title, ax, rotate_xticks=False, palette=None):\n",
    "\n",
    "    percent = df.groupby(category)['cardio'].mean() * 100\n",
    "    sns.barplot(x=percent.index, y=percent.values, ax=ax, palette=palette)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Andel positiva (%)')\n",
    "    if rotate_xticks:\n",
    "        ax.set_xticks(range(len(percent.index)))  # Sätt antalet ticks\n",
    "        ax.set_xticklabels(percent.index, rotation=45)\n",
    "\n",
    "# Skapa en figur med subplots\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Blodtryckskategorier\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_category_vs_cardio(df, 'blood_pressure_category', 'Blodtryckskategorier vs Andel Positiva', \n",
    "                        plt.gca(), rotate_xticks=True, palette='viridis')\n",
    "\n",
    "# Plot 2: BMI-kategorier\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_category_vs_cardio(df, 'BMI_category', 'BMI-kategorier vs Andel Positiva', \n",
    "                        plt.gca(), rotate_xticks=True)\n",
    "\n",
    "# Plot 3: Blodsocker\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_category_vs_cardio(df_gluc, 'gluc_category', 'Blodsocker vs Andel Positiva', \n",
    "                        plt.gca())\n",
    "\n",
    "# Plot 4: Alkohol\n",
    "plt.subplot(2, 2, 4)\n",
    "plot_category_vs_cardio(df, 'alco', 'Alkohol vs Andel Positiva', \n",
    "                        plt.gca())\n",
    "plt.xticks(ticks=[0, 1], labels=['Inte alkoholkonsumtion', 'Alkoholkonsumtion'])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train-validation-test split, punkt 1 i välj modell\n",
    "# Dela upp df1\n",
    "X_df1 = df1.drop('cardio', axis=1) # Feauters\n",
    "y_df1 = df1['cardio'] # Target\n",
    "\n",
    "X_train_df1, X_test_df1, y_train_df1, y_test_df1 = train_test_split(X_df1, y_df1, test_size=0.2, random_state=42)\n",
    "X_train_df1, X_val_df1, y_train_df1, y_val_df1 = train_test_split(X_train_df1, y_train_df1, test_size=0.25, random_state=42)\n",
    "\n",
    "# Dela upp df2\n",
    "X_df2 = df2.drop('cardio', axis=1)\n",
    "y_df2 = df2['cardio']\n",
    "\n",
    "X_train_df2, X_test_df2, y_train_df2, y_test_df2 = train_test_split(X_df2, y_df2, test_size=0.2, random_state=42)\n",
    "X_train_df2, X_val_df2, y_train_df2, y_val_df2 = train_test_split(X_train_df2, y_train_df2, test_size=0.25, random_state=42)\n",
    "\n",
    "# Visa storleken på varje del\n",
    "print(\"Storlekar för df1:\")\n",
    "print(f\"Träningsdata: {X_train_df1.shape}, Valideringsdata: {X_val_df1.shape}, Testdata: {X_test_df1.shape}\")\n",
    "print(\"\\nStorlekar för df2:\")\n",
    "print(f\"Träningsdata: {X_train_df2.shape}, Valideringsdata: {X_val_df2.shape}, Testdata: {X_test_df2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Skala dataset punkt 2 i välj modell\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_df1_std = scaler_standard.fit_transform(X_train_df1)\n",
    "X_val_df1_std = scaler_standard.transform(X_val_df1)\n",
    "X_test_df1_std = scaler_standard.transform(X_test_df1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_df1_norm = scaler_minmax.fit_transform(X_train_df1)\n",
    "X_val_df1_norm = scaler_minmax.transform(X_val_df1)\n",
    "X_test_df1_norm = scaler_minmax.transform(X_test_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Funktion för att träna och evaluera Random Forest\n",
    "def train_evaluate_rf(df):\n",
    "    # Train-validation-test split\n",
    "    X = df.drop('cardio', axis=1)\n",
    "    y = df['cardio']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "    \n",
    "    # Skapa pipeline med skalning och modell\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Definiera hyperparametrar\n",
    "    param_grid = {\n",
    "        'model__n_estimators': [10, 20, 30],\n",
    "        'model__max_depth': [None, 10, 20],\n",
    "        'model__min_samples_split': [2, 5, 10],\n",
    "        'model__min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Prediction och evaluation\n",
    "    y_pred = grid_search.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "    # Returnera resultat\n",
    "    return {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "# Kör Random Forest på båda dataseten\n",
    "result_df1 = train_evaluate_rf(df1)\n",
    "result_df2 = train_evaluate_rf(df2)\n",
    "\n",
    "# Visa resultat\n",
    "print(\"Resultat för df1:\")\n",
    "print(result_df1)\n",
    "print(\"\\nResultat för df2:\")\n",
    "print(result_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Definiera parametergrid, för RandomForestClassifier:\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 20, 30], # Antal träd i skogen\n",
    "    'max_depth': [None, 10, 20, 30], # Maxdjup för varje träd, (None: träden växer tills alla blad är rena.)\n",
    "    'min_samples_split': [2, 5, 10] # Minsta antal sampel som krävs.\n",
    "}\n",
    "\n",
    "# Skapa modell\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Använd funktionen\n",
    "best_rf_model, best_rf_params = perform_grid_search(rf_model, param_grid_rf, X1_train_standard, y1_train)\n",
    "print(\"Bästa parametrar för RandomForest:\", best_rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Definiera parametergrid, för LogisticRegression\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# Skapa modell\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Använd funktionen\n",
    "best_lr_model, best_lr_params = perform_grid_search(lr_model, param_grid_lr, X1_train_standard, y1_train)\n",
    "print(\"Bästa parametrar för LogisticRegression:\", best_lr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Definiera parametergrid för GradienBoostingClassifier\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [3, 5, 10]\n",
    "}\n",
    "\n",
    "# Skapa modell\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Använd GridSearchCV\n",
    "best_gb_model, best_gb_params = perform_grid_search(gb_model, param_grid_gb, X1_train_standard, y1_train)\n",
    "print(\"Bästa parametrar för GradientBoosting:\", best_gb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Definiera parametergrid för KneighborsClassifier\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],  # Antal grannar\n",
    "    'weights': ['uniform', 'distance'],  # Viktning: 'uniform' eller 'distance'\n",
    "    'p': [1,2] # Avståndsmått: 1 för Manhattan, 2 för Euklidiskt avstånd\n",
    "}\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "best_knn_model, best_knn_params = perform_grid_search(knn_model, param_grid_knn, X1_train_standard, y1_train)\n",
    "print(\"Bästa parametrar för KNN:\", best_knn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_and_evaluate_model(model, param_grid, X_train, y_train, X_val, y_val, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Tränar en modell med GridSearchCV, gör förutsägelser på validerings- och testdata,\n",
    "    och returnerar den bästa modellen samt noggrannheten på validerings- och testdata.\n",
    "    \n",
    "    :param model: Maskininlärningsmodellen (t.ex. LogisticRegression).\n",
    "    :param param_grid: Dictionary med hyperparametrar att testa.\n",
    "    :param X_train: Träningsdata (features).\n",
    "    :param y_train: Träningsdata (target).\n",
    "    :param X_val: Valideringsdata (features).\n",
    "    :param y_val: Valideringsdata (target).\n",
    "    :param X_test: Testdata (features).\n",
    "    :param y_test: Testdata (target).\n",
    "    :param model_name: Namn på modellen (för dokumentation).\n",
    "    :return: Den bästa modellen, noggrannhet på valideringsdata, noggrannhet på testdata.\n",
    "    \"\"\"\n",
    "    # Utför GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Hämta den bästa modellen\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Gör förutsägelser på validerings- och testdata\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Beräkna noggrannhet på validerings- och testdata\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    return best_model, val_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Skapa modell\n",
    "log_reg_model = LogisticRegression(random_state=42, solver='liblinear')\n",
    "\n",
    "# Använd funktionen\n",
    "best_log_reg_model, log_reg_val_accuracy, log_reg_test_accuracy = train_and_evaluate_model(\n",
    "    model=log_reg_model,\n",
    "    param_grid=param_grid_logreg,\n",
    "    X_train=X1_train_standard,\n",
    "    y_train=y1_train,\n",
    "    X_val=X1_val_standard,\n",
    "    y_val=y1_val,\n",
    "    X_test=X1_test_standard,\n",
    "    y_test=y1_test,\n",
    "    model_name=\"Logistic Regression\"\n",
    ")\n",
    "\n",
    "\n",
    "# Skapa modell\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Använd funktionen\n",
    "best_rf_model, rf_val_accuracy, rf_test_accuracy = train_and_evaluate_model(\n",
    "    model=rf_model,\n",
    "    param_grid=param_grid_rf,\n",
    "    X_train=X1_train_standard,\n",
    "    y_train=y1_train,\n",
    "    X_val=X1_val_standard,\n",
    "    y_val=y1_val,\n",
    "    X_test=X1_test_standard,\n",
    "    y_test=y1_test,\n",
    "    model_name=\"Random Forest\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Skapa modell för RandomForestClassifier:\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Använd perform_grid_search-funktionen\n",
    "best_rf_model, best_rf_params, rf_metrics = perform_grid_search(\n",
    "    model=rf_model,\n",
    "    param_grid=param_grid_rf,\n",
    "    X_train=X1_train_standard,\n",
    "    y_train=y1_train,\n",
    "    X_val=X1_val_standard,\n",
    "    y_val=y1_val,\n",
    "    model_name= \"Random Forest\"\n",
    ")\n",
    "\n",
    "# Skriv ut resultaten\n",
    "print(\"Bästa RandomForest-modell:\")\n",
    "print(best_rf_model)\n",
    "print(\"\\nBästa parametrar för RandomForest:\")\n",
    "print(best_rf_params)\n",
    "print(\"\\nUtvärderingsmått för RandomForest:\")\n",
    "print(rf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiera parametergrid, för DecisionTreeClassifier\n",
    "param_grid_dt = {\n",
    "    'max_depth': [3, 5, 10],  # Maxdjup för trädet\n",
    "    'min_samples_split': [2, 5, 10],  # Minsta antal sampel för att dela en nod\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minsta antal sampel i ett löv\n",
    "}\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Skapa modell\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Träna och utvärdera DecisionTreeClassifier på df1 - Standardization\n",
    "best_dt_model_df1, best_dt_params_df1, dt_metrics_df1 = perform_grid_search(\n",
    "    model=dt_model,\n",
    "    param_grid=param_grid_dt,\n",
    "    X_train=X1_train_standard,\n",
    "    y_train=y1_train,\n",
    "    X_val=X1_val_standard,\n",
    "    y_val=y1_val,\n",
    "    model_name=\"Decision Tree Standardization (df1)\"\n",
    ")\n",
    "\n",
    "# Träna och utvärdera DecisionTreeClassifier på df2 - Standardization\n",
    "best_dt_model_df2, best_dt_params_df2, dt_metrics_df2 = perform_grid_search(\n",
    "    model=dt_model,\n",
    "    param_grid=param_grid_dt,\n",
    "    X_train=X2_train_standard,\n",
    "    y_train=y2_train,\n",
    "    X_val=X2_val_standard,\n",
    "    y_val=y2_val,\n",
    "    model_name=\"Decision Tree Standardization (df2)\"\n",
    ")\n",
    "\n",
    "# Träna och utvärdera DecisionTreeClassifier på df1 - Normalization\n",
    "best_dt_model_df1_minmax, best_dt_params_df1_minmax, dt_metrics_df1_minmax = perform_grid_search(\n",
    "    model=dt_model,\n",
    "    param_grid=param_grid_dt,\n",
    "    X_train=X1_train_minmax,\n",
    "    y_train=y1_train,\n",
    "    X_val=X1_val_minmax,\n",
    "    y_val=y1_val,\n",
    "    model_name=\"Decision Tree Normalization (df1)\"\n",
    ")\n",
    "\n",
    "# Träna och utvärdera DecisionTreeClassifier på df2 - Normalization\n",
    "best_dt_model_df2_minmax, best_dt_params_df2_minmax, dt_metrics_df2_minmax = perform_grid_search(\n",
    "    model=dt_model,\n",
    "    param_grid=param_grid_dt,\n",
    "    X_train=X2_train_minmax,\n",
    "    y_train=y2_train,\n",
    "    X_val=X2_val_minmax,\n",
    "    y_val=y2_val,\n",
    "    model_name=\"Decision Tree Normalization (df2)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resultat för Decision Tree på standardiserad data (df1):\")\n",
    "print(f\"Bästa parametrar: {best_dt_params_df1}\")\n",
    "print(f\"Noggrannhet: {dt_metrics_df1['accuracy']}\")\n",
    "print(f\"Precision: {dt_metrics_df1['precision']}\")\n",
    "print(f\"Recall: {dt_metrics_df1['recall']}\")\n",
    "print(f\"F1-poäng: {dt_metrics_df1['f1_score']}\")\n",
    "\n",
    "print(\"\\nResultat för Decision Tree på standardiserad data (df2):\")\n",
    "print(f\"Bästa parametrar: {best_dt_params_df2}\")\n",
    "print(f\"Noggrannhet: {dt_metrics_df2['accuracy']}\")\n",
    "print(f\"Precision: {dt_metrics_df2['precision']}\")\n",
    "print(f\"Recall: {dt_metrics_df2['recall']}\")\n",
    "print(f\"F1-poäng: {dt_metrics_df2['f1_score']}\")\n",
    "\n",
    "print(\"\\nResultat för Decision Tree på normaliserad data (df1):\")\n",
    "print(f\"Bästa parametrar: {best_dt_params_df1_minmax}\")\n",
    "print(f\"Noggrannhet: {dt_metrics_df1_minmax['accuracy']}\")\n",
    "print(f\"Precision: {dt_metrics_df1_minmax['precision']}\")\n",
    "print(f\"Recall: {dt_metrics_df1_minmax['recall']}\")\n",
    "print(f\"F1-poäng: {dt_metrics_df1_minmax['f1_score']}\")\n",
    "\n",
    "print(\"\\nResultat för Decision Tree på normaliserad data (df2):\")\n",
    "print(f\"Bästa parametrar: {best_dt_params_df2_minmax}\")\n",
    "print(f\"Noggrannhet: {dt_metrics_df2_minmax['accuracy']}\")\n",
    "print(f\"Precision: {dt_metrics_df2_minmax['precision']}\")\n",
    "print(f\"Recall: {dt_metrics_df2_minmax['recall']}\")\n",
    "print(f\"F1-poäng: {dt_metrics_df2_minmax['f1_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hur jag hade det från början innan nya testet:\n",
    "\n",
    "# Definiera parametergrid för RandomForest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 20, 30], # Antal träd i skogen\n",
    "    'max_depth': [5, 10, 20], # Maxdjup för varje träd.\n",
    "    'min_samples_split': [2, 5, 10, 15], # Minsta antal sampel för att dela en nod\n",
    "    'min_samples_leaf': [1, 2, 4] # Minsta antal sampel i ett löv\n",
    "}\n",
    "\n",
    "# Definiera parametergrid, för LogisticRegression\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10], #Regulariseringsstyrka\n",
    "    'penalty': ['l1', 'l2'], # Strafftyp\n",
    "    'solver': ['saga'] # Optimeringsteknik\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "\n",
    "# Skapar en funktion för att utföra GridsearchCV\n",
    "def perform_grid_search(model, param_grid, X_train, y_train, X_val, y_val, X_test, y_test, model_name, cv=5, scoring='accuracy', save_results=False, filename='model_results.csv'):\n",
    "    \"\"\"\n",
    "    Utför GridSearchCV för en given modell och parametergrid, tränar modellen,\n",
    "    gör förutsägelser på valideringsdata och beräknar utvärderingsmått.\n",
    "    \n",
    "    :param model: Maskininlärningsmodellen (t.ex. RandomForestClassifier).\n",
    "    :param param_grid: Dictionary med hyperparametrar att testa.\n",
    "    :param X_train: Träningsdata (features).\n",
    "    :param y_train: Träningsdata (target).\n",
    "    :param X_val: Valideringsdata (features).\n",
    "    :param y_val: Valideringsdata (target).\n",
    "    :param X_test: Testdata (features).\n",
    "    :param y_test: Testdata (target).\n",
    "    :param cv: Antal folds för korsvalidering (default: 5).\n",
    "    :param scoring: Mätvärde för att välja bästa modell (default: 'accuracy').\n",
    "    :param save_results: Om True, spara resultaten till en CSV-fil (default: False).\n",
    "    :param filename: Namnet på CSV-filen (default: 'model_results.csv').\n",
    "    :return: Den bästa modellen, dess parametrar och utvärderingsmått.\n",
    "    \"\"\"\n",
    "    # Utför GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring=scoring)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Hämta den bästa modellen och dess parametrar\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Gör förutsägelser på valideringsdata\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Beräkna utvärderingsmått\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_precision = precision_score(y_val, y_val_pred)\n",
    "    val_recall = recall_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Skapa resultaten\n",
    "    evaluation_metrics = {\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'val_precision': val_precision,\n",
    "        'val_recall': val_recall,\n",
    "        'val_f1': val_f1,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'test_f1': test_f1\n",
    "    }\n",
    "    \n",
    "    # Spara resultaten om save_results är True\n",
    "    if save_results:\n",
    "        results = {\n",
    "            'Model': [model_name],\n",
    "            'Best Parameters': [str(best_params)],\n",
    "            'Validation Accuracy': [val_accuracy],\n",
    "            'Test Accuracy': [test_accuracy],\n",
    "            'Validation Precision': [val_precision],\n",
    "            'Test Precision': [test_precision],\n",
    "            'Validation Recall': [val_recall],\n",
    "            'Test Recall': [test_recall],\n",
    "            'Validation F1': [val_f1],\n",
    "            'Test F1': [test_f1]\n",
    "        }\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        # Kontrollera om filen redan finns\n",
    "        if os.path.exists(filename):\n",
    "            existing_df = pd.read_csv(filename)\n",
    "            # Kontrollera om modellen redan finns i filen\n",
    "            if model_name not in existing_df['Model'].values:\n",
    "                # Lägg till nya resultat om modellen inte redan finns\n",
    "                results_df.to_csv(filename, mode='a', header=False, index=False)\n",
    "                print(f\"Nya resultat för '{model_name}' sparades i {filename}.\")\n",
    "            else:\n",
    "                print(f\"Modellen '{model_name}' finns redan i filen. Inga nya resultat sparades.\")\n",
    "        else:\n",
    "            # Skapa en ny fil om den inte finns\n",
    "            results_df.to_csv(filename, mode='w', header=True, index=False)\n",
    "            print(f\"Ny fil skapad: {filename}\")\n",
    "    \n",
    "    return best_model, best_params, evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Skapa modeller med de bästa parametrarna från tidigare resultat för df2\n",
    "    rf_best = RandomForestClassifier(\n",
    "        max_depth=10, \n",
    "        min_samples_leaf=4, \n",
    "        min_samples_split=10, \n",
    "        n_estimators=30,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    lr_best = LogisticRegression(\n",
    "        C=100, \n",
    "        penalty='l2', \n",
    "        solver='saga',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    gb_best = GradientBoostingClassifier(\n",
    "        learning_rate=0.1, \n",
    "        max_depth=5, \n",
    "        n_estimators=30,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    knn_best = KNeighborsClassifier(\n",
    "        n_neighbors=9, \n",
    "        p=1, \n",
    "        weights='uniform'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "# Ensemble modell med VotingClassifier\n",
    "def create_ensemble_model(X2_train_minmax, y2_train, X2_val_minmax, y2_val, X2_test_minmax, y2_test, dataset_name=\"df2\"):\n",
    "    \"\"\"\n",
    "    Skapar en ensemble-modell med VotingClassifier baserat på de bästa parametrarna\n",
    "    från tidigare träning. Använder 'hard' voting (majoritetsröstning).\n",
    "    \n",
    "    Args:\n",
    "        X2_train_minmax, y2_train: Träningsdata\n",
    "        X2_val_minmax, y2_val: Valideringsdata\n",
    "        X2_test_minmax, y2_test: Testdata\n",
    "        dataset_name: Namnet på datasetet (default: \"df2\")\n",
    "    \n",
    "    Returns:\n",
    "        ensemble_model: Den tränade ensemble-modellen\n",
    "        accuracy: Noggrannhet på testdata\n",
    "    \"\"\"\n",
    "    # Ladda resultat från CSV-filen\n",
    "    results_df = pd.read_csv('model_results.csv')\n",
    "    \n",
    "    # Filtrera resultat specifikt för df2\n",
    "    df2_results = results_df[results_df['Model'].str.contains(dataset_name)]\n",
    "    \n",
    "    # Extrahera bästa parametrar för varje modell\n",
    "    rf_params = eval(df2_results[df2_results['Model'].str.contains('Random Forest')]['Best Parameters'].values[0])\n",
    "    lr_params = eval(df2_results[df2_results['Model'].str.contains('Logistic Regression')]['Best Parameters'].values[0])\n",
    "    gb_params = eval(df2_results[df2_results['Model'].str.contains('Gradient Boosting')]['Best Parameters'].values[0])\n",
    "    knn_params = eval(df2_results[df2_results['Model'].str.contains('KNN')]['Best Parameters'].values[0])\n",
    "    \n",
    "    # Lägg till random_state för modeller som stödjer det\n",
    "    rf_params['random_state'] = 42\n",
    "    lr_params['random_state'] = 42\n",
    "    gb_params['random_state'] = 42\n",
    "    \n",
    "    # Skapa modeller med de extraherade parametrarna\n",
    "    rf_best = RandomForestClassifier(**rf_params)\n",
    "    lr_best = LogisticRegression(**lr_params)\n",
    "    gb_best = GradientBoostingClassifier(**gb_params)\n",
    "    knn_best = KNeighborsClassifier(**knn_params)\n",
    "    \n",
    "    # Kombinera tränings- och valideringsdata för att träna den slutliga modellen\n",
    "    X2_train_combined = pd.concat([pd.DataFrame(X2_train_minmax), pd.DataFrame(X2_val_minmax)])\n",
    "    y2_train_combined = pd.concat([pd.Series(y2_train), pd.Series(y2_val)])\n",
    "    \n",
    "    # Skapa VotingClassifier\n",
    "    ensemble_model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', rf_best),\n",
    "            ('lr', lr_best),\n",
    "            ('gb', gb_best),\n",
    "            ('knn', knn_best)\n",
    "        ],\n",
    "        voting='hard'  # Använd majoritetsröstning\n",
    "    )\n",
    "    \n",
    "    # Träna ensemble-modellen på kombinerad tränings- och valideringsdata\n",
    "    print(f\"Tränar ensemble-modell med VotingClassifier på {dataset_name}...\")\n",
    "    \n",
    "    ensemble_model.fit(X2_train_combined, y2_train_combined)\n",
    "    \n",
    "    # Skapa en dictionary för att lagra individuella modeller\n",
    "    individual_models = {\n",
    "        'Random Forest': rf_best,\n",
    "        'Logistic Regression': lr_best,\n",
    "        'Gradient Boosting': gb_best,\n",
    "        'KNN': knn_best\n",
    "    }\n",
    "    \n",
    "    # Utvärdera individuella modeller och ensemble-modellen\n",
    "    results = {'Modell': [], 'Noggrannhet': []}\n",
    "    \n",
    "    # Utvärdera individuella modeller\n",
    "    print(\"\\nUtvärderar individuella modeller på df2-datasetet...\")\n",
    "    for name, model in individual_models.items():\n",
    "        model.fit(X2_train_combined, y2_train_combined)\n",
    "        y2_pred = model.predict(X2_test_minmax)\n",
    "        acc = accuracy_score(y2_test, y2_pred)\n",
    "        results['Modell'].append(name)\n",
    "        results['Noggrannhet'].append(acc)\n",
    "        print(f\"{name}: {acc:.4f}\")\n",
    "    \n",
    "    # Utvärdera ensemble-modellen\n",
    "    y2_test_pred = ensemble_model.predict(X2_test_minmax)\n",
    "    test_accuracy = accuracy_score(y2_test, y2_test_pred)\n",
    "    \n",
    "    # Lägg till ensemble-resultatet\n",
    "    results['Modell'].append('Ensemble (Voting)')\n",
    "    results['Noggrannhet'].append(test_accuracy)\n",
    "    \n",
    "    print(f\"Ensemble-modell (VotingClassifier) på {dataset_name}:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Ta bort den gamla filen om den finns\n",
    "    filename = \"ensemble_model_results_df2.csv\"\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            os.remove(filename)\n",
    "            print(f\"Gammal fil borttagen: {filename}\")\n",
    "        except:\n",
    "            print(f\"Kunde inte ta bort gammal fil: {filename}\")\n",
    "    \n",
    "    # Spara resultaten till CSV med det nya formatet\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"Resultat sparade i {filename} med nytt format (Modell, Noggrannhet)\")\n",
    "    \n",
    "    return ensemble_model, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ensemble_model_df2 import create_ensemble_model\n",
    "\n",
    "ensemble_model, test_accuracy = create_ensemble_model(\n",
    "    X2_train_minmax,\n",
    "    y2_train,\n",
    "    X2_val_minmax,\n",
    "    y2_val,\n",
    "    X2_test_minmax,\n",
    "    y2_test,\n",
    "    dataset_name=\"df2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def train_ensemble_model_df2(X2_train_minmax, X2_val_minmax, X2_test_minmax, \n",
    "                            y2_train, y2_val, y2_test, \n",
    "                            save_results=True, filename='ensemble_model_results_df2.csv'):\n",
    "    \"\"\"\n",
    "    Tränar en ensemble-modell med VotingClassifier för df2-datasetet.\n",
    "    \n",
    "    Parametrar:\n",
    "    -----------\n",
    "    X2_train_minmax : array-like\n",
    "        Träningsdata för df2, normaliserad med MinMaxScaler\n",
    "    X2_val_minmax : array-like\n",
    "        Valideringsdata för df2, normaliserad med MinMaxScaler\n",
    "    X2_test_minmax : array-like\n",
    "        Testdata för df2, normaliserad med MinMaxScaler\n",
    "    y2_train : array-like\n",
    "        Målvariabel för träningsdata\n",
    "    y2_val : array-like\n",
    "        Målvariabel för valideringsdata\n",
    "    y2_test : array-like\n",
    "        Målvariabel för testdata\n",
    "    save_results : bool, default=True\n",
    "        Om True, sparas resultaten i en CSV-fil\n",
    "    filename : str, default='ensemble_model_results_df2.csv'\n",
    "        Filnamn för att spara resultaten\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ensemble_model : VotingClassifier\n",
    "        Den tränade ensemble-modellen\n",
    "    \"\"\"\n",
    "    \n",
    "    # Kombinera tränings- och valideringsdata\n",
    "    X_train_val = np.vstack((X2_train_minmax, X2_val_minmax))\n",
    "    y_train_val = np.concatenate((y2_train, y2_val))\n",
    "    \n",
    "    # Skapa individuella modeller med bästa parametrar\n",
    "    rf_model = RandomForestClassifier(\n",
    "        max_depth=10, \n",
    "        min_samples_leaf=4, \n",
    "        min_samples_split=10, \n",
    "        n_estimators=30,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    lr_model = LogisticRegression(\n",
    "        C=100, \n",
    "        penalty='l2', \n",
    "        solver='saga',\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    \n",
    "    gb_model = GradientBoostingClassifier(\n",
    "        learning_rate=0.1, \n",
    "        max_depth=5, \n",
    "        n_estimators=30,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    knn_model = KNeighborsClassifier(\n",
    "        n_neighbors=9, \n",
    "        p=1, \n",
    "        weights='uniform'\n",
    "    )\n",
    "    \n",
    "    # Träna individuella modeller på kombinerad tränings- och valideringsdata\n",
    "    rf_model.fit(X_train_val, y_train_val)\n",
    "    lr_model.fit(X_train_val, y_train_val)\n",
    "    gb_model.fit(X_train_val, y_train_val)\n",
    "    knn_model.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    # Skapa ensemble-modell med VotingClassifier\n",
    "    ensemble_model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', rf_model),\n",
    "            ('lr', lr_model),\n",
    "            ('gb', gb_model),\n",
    "            ('knn', knn_model)\n",
    "        ],\n",
    "        voting='soft'  # Använd sannolikheter för röstning\n",
    "    )\n",
    "    \n",
    "    # Träna ensemble-modellen på kombinerad tränings- och valideringsdata\n",
    "    ensemble_model.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    # Utvärdera modellerna på testdata\n",
    "    rf_pred = rf_model.predict(X2_test_minmax)\n",
    "    lr_pred = lr_model.predict(X2_test_minmax)\n",
    "    gb_pred = gb_model.predict(X2_test_minmax)\n",
    "    knn_pred = knn_model.predict(X2_test_minmax)\n",
    "    ensemble_pred = ensemble_model.predict(X2_test_minmax)\n",
    "    \n",
    "    # Beräkna noggrannhet\n",
    "    rf_accuracy = accuracy_score(y2_test, rf_pred)\n",
    "    lr_accuracy = accuracy_score(y2_test, lr_pred)\n",
    "    gb_accuracy = accuracy_score(y2_test, gb_pred)\n",
    "    knn_accuracy = accuracy_score(y2_test, knn_pred)\n",
    "    ensemble_accuracy = accuracy_score(y2_test, ensemble_pred)\n",
    "    \n",
    "    # Skapa resultatdataframe\n",
    "    if save_results:\n",
    "        results = {\n",
    "            'Modell': ['Random Forest', 'Logistic Regression', 'Gradient Boosting', 'KNN', 'Ensemble (Voting)'],\n",
    "            'Noggrannhet': [rf_accuracy, lr_accuracy, gb_accuracy, knn_accuracy, ensemble_accuracy]\n",
    "        }\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(filename, index=False)\n",
    "        print(f\"Resultat sparade i {filename}\")\n",
    "        \n",
    "        # Skriv ut resultat\n",
    "        print(\"\\nResultat på testdata:\")\n",
    "        print(f\"Random Forest noggrannhet: {rf_accuracy:.4f}\")\n",
    "        print(f\"Logistic Regression noggrannhet: {lr_accuracy:.4f}\")\n",
    "        print(f\"Gradient Boosting noggrannhet: {gb_accuracy:.4f}\")\n",
    "        print(f\"KNN noggrannhet: {knn_accuracy:.4f}\")\n",
    "        print(f\"Ensemble (Voting) noggrannhet: {ensemble_accuracy:.4f}\")\n",
    "    \n",
    "    return ensemble_model\n",
    "\n",
    "# Exempel på användning:\n",
    "# från Ensemble_model_df2 import train_ensemble_model_df2\n",
    "# ensemble_model = train_ensemble_model_df2(X2_train_minmax, X2_val_minmax, X2_test_minmax, y2_train, y2_val, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importera ensemble-modellen\n",
    "from Ensemble_model_df2 import train_ensemble_model_df2\n",
    "\n",
    "# Kombinera tränings- och valideringsdata för att träna den slutliga modellen\n",
    "ensemble_model = train_ensemble_model_df2(\n",
    "    X2_train_minmax, X2_val_minmax, X2_test_minmax, \n",
    "    y2_train, y2_val, y2_test\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
